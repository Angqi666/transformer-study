{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localscratch/liangqi1/miniconda3/envs/transformer/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-30e904a938ab>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoded_texts = torch.load(save_path)\n"
     ]
    }
   ],
   "source": [
    "# Using bert-base-chinese as tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "save_path = '/egr/research-slim/liangqi1/LLM/transformer-study/data/encoded_texts.pt'  # 替换为你的文件路径\n",
    "encoded_texts = torch.load(save_path)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size  # 21128（BERT 预训练词汇表大小）\n",
    "embedding_dim = 256  # 768 维度（和 BERT 一致）\n",
    "\n",
    "# 定义嵌入层\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "sample = encoded_texts.input_ids[0]\n",
    "embedded_tokens = embedding_layer(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Positional Encoding** is a **(seq_len, embedding_dim)** shaped matrix used for encoding token positions.  \n",
    "✅ Its calculation is based on **sin/cos functions**, ensuring the model can learn both **short-term and long-term dependencies**.  \n",
    "✅ The **embedding output of the Transformer** is added with **Positional Encoding**, allowing the model to perceive **token order information**.  \n",
    "✅ The **code example computes a (512, 256) Positional Encoding** and adds it to the embedding result.  \n",
    "✅ **Positional Encoding visualization** helps observe the pattern of **sin/cos values changing with token positions**.\n",
    "\n",
    "---\n",
    "\n",
    "✅ Positional Encoding 是一个 (seq_len, embedding_dim) 形状的矩阵，用于为 token 位置编码。<br>\n",
    "✅ 它的计算基于 sin/cos 函数，确保模型能学习短期和长期依赖关系。<br>\n",
    "✅ Transformer 的 embedding 结果会加上 Positional Encoding，使模型感知到 token 的顺序信息。<br>\n",
    "✅ 代码示例计算了 (512, 256) 的 Positional Encoding 并将其加到 embedding 结果上。<br>\n",
    "✅ 可视化 Positional Encoding 变化，观察随 token 位置变化的 sin/cos 规律。<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Output = Embedded Tokens (512, 256) + Positional Encoding (512, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the positions: torch.Size([512, 1])\n",
      "Positional Encoding Shape: torch.Size([512, 256])\n",
      "Final Embedding Shape: torch.Size([512, 256])\n"
     ]
    }
   ],
   "source": [
    "# 创建 shape 为 (seq_len, embedding_dim) 的空矩阵\n",
    "seq_len = embedded_tokens.shape[0]\n",
    "embedding_dim = embedded_tokens.shape[1]\n",
    "\n",
    "pe = torch.zeros(seq_len, embedding_dim)\n",
    "\n",
    "# 生成位置索引 (pos) → shape: (seq_len, 1)\n",
    "position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "print(f'shape of the positions: {position.shape}')\n",
    "\n",
    "# 计算每个维度的分母 10000^(-2i/d)\n",
    "# Exponentiation is faster than power arithmetic (exp is more efficient than pow).\n",
    "# Floating point errors can be avoided (preventing overflow if 10000^{-x} is too small).\n",
    "div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
    "\n",
    "# 偶数索引使用 sin，奇数索引使用 cos\n",
    "pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度\n",
    "pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度\n",
    "\n",
    "print(\"Positional Encoding Shape:\", pe.shape)  # (512, 256)\n",
    "\n",
    "\n",
    "final_embedding = embedded_tokens + pe\n",
    "print(\"Final Embedding Shape:\", final_embedding.shape)  # (512, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
